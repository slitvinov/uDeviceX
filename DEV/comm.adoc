= comm

generic communicator with "halo".

== purpose

== data structures


* `hBags`: buffers on the host, contains all necessary information of the data to communicate:
** `data`: host buffer containing the data
** `counts`: number of items per fragment
** `capacity`: maximum number of items of buffers `data`
** `bsize`: size (in bytes) of one item
* `dBags`: buffers on the device
* `Stamp`: contains the communication related variables:
** `sreq`, `rreq`: send and receive requests
** `bt`: base tag: tag of one exchange is `bt + fid`, where `fid` is the fragment id
** `cart`: cartesian communicator
** `ranks`: ranks of the neighbors in the grid (who do I send to?)
** `tags`: tags used by neighbors to send messages
  
== allocation mod

User can choose how the buffers are allocated with the enum type `AllocMod`.
This is used in the funcion `ini` and `fin`. allocation mode and free mode are assumed to be the same.  

currently supported allocation modes:
[source,c++]
----
HST_ONLY,   /* only host bags allocated                 */
DEV_ONLY,   /* only device bags allocated               */
PINNED,     /* both host and device pinned              */
PINNED_DEV, /* host pinned; device global memory on gpu */
NONE        /* no allocation                            */
----

== interface

=== ini

[source,c++]
----
void ini(AllocMod fmod, AllocMod bmod, size_t bsize, const int capacity[NBAGS], /**/ hBags *hb, dBags *db); //<1>
void ini(MPI_Comm comm, /*io*/ basetags::TagGen *tg, /**/ Stamp *s); //<2>
----

<1> Given two structures `hBags` and `dBags`, `ini` allocates the buffers on host and device. `ini` expects 2 allocation modes:
* `fmod`: allocation mode for fragment buffers
* `bmod`: allocation mode for bulk buffer

<2> initialize `Stamp` structure

=== fin

free memory allocated by `ini`

=== communication

[source,c++]
----
void post_recv(hBags *b, Stamp *s); //<1>
void post_send(const hBags *b, Stamp *s);  //<2>

void wait_recv(Stamp *s, /**/ hBags *b); //<3>
void wait_send(Stamp *s); //<4>
----

<1> MPI_IRecv
<2> MPI_ISend
<3> wait for recv
<4> wait for send
